import React, { useState, useEffect, useCallback, useRef, useMemo } from 'react';
import { Volume2, Mic, MicOff, Activity } from 'lucide-react';
import { voiceDetectionService } from '../../services/audio/voiceDetectionService';
import { audioService } from '../../services/audio/audioService';
import { useVoiceLevel } from '../../hooks/useVoiceLevel';
import { ScrollingText } from './ScrollingText';

interface Message {
  id: string;
  sessionId: string;
  content: string;
  speaker: 'user' | 'ai';
  timestamp: Date;
  audioUrl?: string;
  isSpoken?: boolean;
  isSpeaking?: boolean;
  metadata?: {
    sentiment?: number;
    confidence?: number;
    duration?: number;
  };
}

interface VoiceAssistantProps {
  messages: Message[];
  onSendMessage: (content: string) => void;
  sessionId: string;
  disabled?: boolean;
}

interface AssistantState {
  isListening: boolean;
  isThinking: boolean;
  isAISpeaking: boolean;
  lastUserMessage: string;
  isActive: boolean;
  currentAIText: string;
}

export const VoiceAssistant: React.FC<VoiceAssistantProps> = ({
  messages,
  onSendMessage,
  sessionId,
  disabled = false
}) => {
  const [state, setState] = useState<AssistantState>({
    isListening: false,
    isThinking: false,
    isAISpeaking: false,
    lastUserMessage: '',
    isActive: false,
    currentAIText: ''
  });

  const [currentTranscript, setCurrentTranscript] = useState('');
  const stateRef = useRef(state);
  stateRef.current = state;

  // Utiliser le hook de dÃ©tection de volume rÃ©el
  const { volumeLevel } = useVoiceLevel({
    enabled: state.isListening,
    onUpdate: (level) => {
      // Le volume est dÃ©jÃ  mis Ã  jour par le hook
    }
  });

  // DÃ©tecter quand l'IA parle
  useEffect(() => {
    const latestMessage = messages[messages.length - 1];
    console.log("ğŸ” Debug - Latest message:", latestMessage);

    if (latestMessage?.speaker === 'ai') {
      if (latestMessage.isSpeaking) {
        // L'IA commence Ã  parler : arrÃªter IMMÃ‰DIATEMENT la reconnaissance
        console.log("ğŸ”Š L'IA commence Ã  parler - arrÃªt de la reconnaissance");
        console.log("ğŸ“ Texte IA:", latestMessage.content);
        setState(prev => ({
          ...prev,
          isAISpeaking: true,
          isListening: false,
          currentAIText: latestMessage.content,
          isThinking: false
        }));
        voiceDetectionService.stopDetection();
      } else if (latestMessage.isSpoken) {
        // L'IA a terminÃ© de parler : rÃ©activer la reconnaissance aprÃ¨s un dÃ©lai
        console.log("ğŸ”‡ L'IA a terminÃ© de parler - rÃ©activation de la reconnaissance");
        setState(prev => ({
          ...prev,
          isAISpeaking: false,
          currentAIText: ''
        }));
      }
    }
  }, [messages]);

  // Callbacks pour la dÃ©tection vocale
  const voiceCallbacks = useMemo(() => ({
    onSpeechStart: () => {
      console.log("ğŸ¤ Speech started - Beginning recording");
      setState(prev => ({
        ...prev,
        isListening: true,
        isActive: true
      }));

      // DÃ©marrer l'enregistrement audio
      audioService.startRecording().catch(console.error);
    },

    onSpeechEnd: async (transcript: string) => {
      console.log("ğŸ¯ Speech ended - Processing:", transcript);

      // ArrÃªter l'enregistrement audio
      try {
        await audioService.stopRecording();
      } catch (error) {
        console.warn("Error stopping audio recording:", error);
      }

      // Nettoyer la transcription
      const cleanedTranscript = transcript.trim()
        .replace(/^(euh|ah|m|h|hum|hein|donc|ben|alors)\s+/gi, '')
        .replace(/\s+(euh|ah|m|h|hum|hein|donc|ben|alors)$/gi, '')
        .replace(/\s+/g, ' ')
        .trim();

      if (cleanedTranscript.length > 2) {
        setState(prev => ({
          ...prev,
          isListening: false,
          isThinking: true,
          lastUserMessage: cleanedTranscript
        }));

        setCurrentTranscript('');

        // Envoyer le message Ã  l'IA
        onSendMessage(cleanedTranscript);
      } else {
        setState(prev => ({ ...prev, isListening: false }));
      }
    },

    onSilenceTimeout: () => {
      console.log("â° Silence detected");
      setState(prev => ({ ...prev, isListening: false }));
    },

    onError: (error: string) => {
      console.error("Voice detection error:", error);
      setState(prev => ({
        ...prev,
        isListening: false,
        isThinking: false
      }));
    }
  }), [onSendMessage]);

  // DÃ©marrer/arrÃªter la dÃ©tection vocale automatique
  useEffect(() => {
    if (disabled) {
      voiceDetectionService.stopDetection();
      return;
    }

    // IMPORTANT : DÃ©sactiver TOTALEMENT la reconnaissance quand l'IA parle ou rÃ©flÃ©chit
    // pour Ã©viter de transcrire la voix de l'IA
    if (state.isAISpeaking || state.isThinking || !state.isActive) {
      voiceDetectionService.stopDetection();
      return;
    }

    // Si l'IA ne parle pas et qu'on n'est pas en train de rÃ©flÃ©chir, activer la dÃ©tection
    if (state.isActive && !state.isAISpeaking && !state.isThinking) {
      voiceDetectionService.startDetection(voiceCallbacks);
    }

    return () => {
      voiceDetectionService.stopDetection();
    };
  }, [disabled, state.isAISpeaking, state.isThinking, state.isActive]);

  // Mettre Ã  jour le transcript en temps rÃ©el
  useEffect(() => {
    const updateTranscript = () => {
      const transcript = voiceDetectionService.getCurrentTranscript();
      if (transcript !== currentTranscript) {
        setCurrentTranscript(transcript);
      }
    };

    const interval = setInterval(updateTranscript, 100);
    return () => clearInterval(interval);
  }, [currentTranscript]);

  // Activer/dÃ©sactiver manuellement l'assistant
  const toggleAssistant = useCallback(() => {
    if (state.isActive) {
      voiceDetectionService.stopDetection();
      setState(prev => ({
        ...prev,
        isActive: false,
        isListening: false,
        isThinking: false
      }));
    } else {
      setState(prev => ({ ...prev, isActive: true }));
      voiceDetectionService.startDetection(voiceCallbacks);
    }
  }, [state.isActive, voiceCallbacks]);

  
  const getStatusColor = () => {
    if (state.isThinking) return 'text-yellow-600';
    if (state.isAISpeaking) return 'text-blue-600';
    if (state.isListening) return 'text-green-600';
    if (state.isActive) return 'text-gray-600';
    return 'text-gray-400';
  };

  const getStatusText = () => {
    if (state.isThinking) return 'ğŸ¤” RÃ©flexion...';
    if (state.isAISpeaking) return 'ğŸ”Š L\'IA parle...';
    if (state.isListening) return 'ğŸ¤ J\'Ã©coute...';
    if (state.isActive) return 'ğŸ‘‚ En attente...';
    return 'ğŸ˜´ Inactif';
  };

  const getStatusIcon = () => {
    if (state.isThinking) return <Activity className="w-6 h-6 animate-pulse" />;
    if (state.isAISpeaking) return <Volume2 className="w-6 h-6 animate-pulse" />;
    if (state.isListening) return <Mic className="w-6 h-6 animate-pulse" />;
    return <MicOff className="w-6 h-6" />;
  };

  return (
    <div className={`bg-white rounded-lg border ${state.isActive ? 'border-gray-300' : 'border-gray-200'} p-6 transition-all duration-300`}>
      {/* Header */}
      <div className="flex items-center justify-between mb-4">
        <div className="flex items-center space-x-3">
          <div className={`${getStatusColor()}`}>
            {getStatusIcon()}
          </div>
          <div>
            <h3 className="font-semibold text-gray-800">Assistant Vocal</h3>
            <p className={`text-sm ${getStatusColor()}`}>
              {getStatusText()}
            </p>
          </div>
        </div>

        <button
          onClick={toggleAssistant}
          disabled={disabled}
          className={`px-4 py-2 rounded-lg font-medium transition-colors ${
            state.isActive
              ? 'bg-green-500 text-white hover:bg-green-600'
              : 'bg-gray-200 text-gray-600 hover:bg-gray-300'
          } ${disabled ? 'opacity-50 cursor-not-allowed' : ''}`}
        >
          {state.isActive ? 'ACTIVÃ‰' : 'ACTIVER'}
        </button>
      </div>

      {/* Indicateur de volume */}
      {state.isListening && (
        <div className="mb-4">
          <div className="flex items-center space-x-2 mb-2">
            <span className="text-sm text-gray-600">Volume</span>
            <div className="flex-1 bg-gray-200 rounded-full h-2">
              <div
                className="bg-green-500 h-2 rounded-full transition-all duration-200"
                style={{ width: `${volumeLevel}%` }}
              />
            </div>
          </div>
        </div>
      )}

      {/* Transcript en temps rÃ©el */}
      {currentTranscript && (
        <div className="mb-4 p-3 bg-gray-50 rounded-lg">
          <p className="text-sm text-gray-600 mb-1">Je comprends :</p>
          <p className="text-gray-800 font-medium">{currentTranscript}</p>
        </div>
      )}

      {/* Dernier message utilisateur */}
      {state.lastUserMessage && (
        <div className="mb-4 p-3 bg-blue-50 rounded-lg">
          <p className="text-sm text-blue-600 mb-1">Vous avez dit :</p>
          <p className="text-blue-800 font-medium">{state.lastUserMessage}</p>
        </div>
      )}

      {/* Texte dÃ©filant de l'IA */}
      <div className="mb-4">
        <p className="text-sm text-gray-600 mb-2">
          {state.isThinking ? "ğŸ¤” L'IA rÃ©flÃ©chit..." : state.isAISpeaking ? "ğŸ”Š L'IA parle :" : "ğŸ’­ IA prÃªte"}
        </p>
        <ScrollingText
          text={state.isThinking ? "En train de rÃ©flÃ©chir Ã  votre question..." : state.currentAIText || "Attente de votre message..."}
          isActive={state.isAISpeaking}
          speed={60}
        />
      </div>

      {/* Instructions */}
      <div className="text-xs text-gray-500 space-y-1">
        <p>â€¢ Parlez naturellement, l'assistant vous Ã©coute en continu</p>
        <p>â€¢ L'assistant s'arrÃªte automatiquement aprÃ¨s 2 secondes de silence</p>
        <p>â€¢ Vous pouvez interrompre l'IA en commenÃ§ant Ã  parler</p>
        <p>â€¢ Cliquez sur "ACTIVER" pour dÃ©marrer l'assistant</p>
      </div>
    </div>
  );
};